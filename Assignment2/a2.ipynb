{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from load_data import get_training_data, get_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_training_data()\n",
    "test_data = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = randint(0, len(training_data[\"data\"]) - 1)\n",
    "img = training_data[\"data\"][random_index]\n",
    "\n",
    "# Separate the color channels\n",
    "red = img[0:1024].reshape((32, 32))\n",
    "green = img[1024:2048].reshape((32, 32))\n",
    "blue = img[2048:3072].reshape((32, 32))\n",
    "\n",
    "# Stack channels along the third dimension\n",
    "img_rgb = np.stack((red, green, blue), axis=2)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:52:21.187737: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 16:52:22.417114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730269342.909920   30026 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730269343.052765   30026 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 16:52:24.422807: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:52:42.223668: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (100)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "my_new_model = Sequential()\n",
    "my_new_model.add(VGG16(include_top=False, pooling='avg', weights='imagenet'))\n",
    "my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Indicate whether the first layer should be trained/changed or not.\n",
    "my_new_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_model.compile(optimizer='sgd', \n",
    "                     loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "def data_generator(data, labels, batch_size=32):\n",
    "    num_samples = data.shape[0]\n",
    "\n",
    "    while True:  # Loop forever so the generator never terminates\n",
    "        # Shuffle the data at the start of each epoch\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "\n",
    "            # Initialize batch arrays\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for i in batch_indices:\n",
    "                # Reshape each image from (3072,) to (32, 32, 3)\n",
    "                img = data[i]\n",
    "                red = img[0:1024].reshape((32, 32))\n",
    "                green = img[1024:2048].reshape((32, 32))\n",
    "                blue = img[2048:3072].reshape((32, 32))\n",
    "                img_rgb = np.stack((red, green, blue), axis=-1)\n",
    "\n",
    "                # Resize to (224, 224) for ResNet50\n",
    "                img_resized = resize(img_rgb, (image_size, image_size)).numpy()\n",
    "\n",
    "                # Preprocess the image\n",
    "                img_preprocessed = preprocess_input(img_resized)\n",
    "\n",
    "                # Append to batch\n",
    "                batch_images.append(img_preprocessed)\n",
    "                batch_labels.append(labels[i])\n",
    "\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_generator = data_generator(training_data[\"data\"], training_data[\"labels\"], batch_size=10)\n",
    "validation_generator = data_generator(test_data[\"data\"], test_data[\"labels\"], batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def memory_efficient_generator(data, labels, batch_size=32):\n",
    "    num_samples = len(data)\n",
    "    \n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for i in batch_indices:\n",
    "                img = data[i]\n",
    "                # Reshape and resize\n",
    "                red = img[0:1024].reshape((32, 32))\n",
    "                green = img[1024:2048].reshape((32, 32))\n",
    "                blue = img[2048:3072].reshape((32, 32))\n",
    "                img_rgb = np.stack((red, green, blue), axis=-1)\n",
    "                img_resized = resize(img_rgb, (image_size, image_size)).numpy()\n",
    "                img_preprocessed = preprocess_input(img_resized)\n",
    "                \n",
    "                batch_images.append(img_preprocessed)\n",
    "                batch_labels.append(labels[i])\n",
    "            \n",
    "            yield np.array(batch_images), tf.keras.utils.to_categorical(batch_labels, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_stats below saves some statistics describing how model fitting went\n",
    "# the key role of the following line is how it changes my_new_model by fitting to data\n",
    "# train_generator = memory_efficient_generator(training_data[\"data\"], training_data[\"labels\"], batch_size=22)\n",
    "# my_new_model.fit(train_generator, steps_per_epoch=len(training_data[\"data\"]) // 10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mCannot take the length of shape with unknown rank.\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Now use train_dataset in fit\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmy_new_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/chris/dev/repos/COMP-SCI-7318-Deep-Learning-Fundametals/Assignment2/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/mnt/c/Users/chris/dev/repos/COMP-SCI-7318-Deep-Learning-Fundametals/Assignment2/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mCannot take the length of shape with unknown rank.\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Prepare dataset directly from the original images and labels\n",
    "def preprocess_image(img, label):\n",
    "    red = img[0:1024].reshape((32, 32))\n",
    "    green = img[1024:2048].reshape((32, 32))\n",
    "    blue = img[2048:3072].reshape((32, 32))\n",
    "    img_rgb = np.stack((red, green, blue), axis=-1)\n",
    "    \n",
    "    # Resize and preprocess\n",
    "    img_resized = tf.image.resize(img_rgb, (image_size, image_size))\n",
    "    img_preprocessed = preprocess_input(img_resized)\n",
    "    \n",
    "    return img_preprocessed, tf.keras.utils.to_categorical(label, num_classes=num_classes)\n",
    "\n",
    "# Create the tf.data.Dataset for training data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_data[\"data\"], training_data[\"labels\"]))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(training_data[\"data\"]))\n",
    "train_dataset = train_dataset.map(lambda img, label: tf.py_function(preprocess_image, [img, label], [tf.float32, tf.float32]),\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Now use train_dataset in fit\n",
    "my_new_model.fit(train_dataset, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
